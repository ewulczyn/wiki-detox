{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78.4 ms\n"
     ]
    }
   ],
   "source": [
    "import inspect, os\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "os.sys.path.insert(0,parentdir) \n",
    "from data_generation.diff_utils import clean_and_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Models Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "from baselines import plurality, average\n",
    "from serialization import load_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "all_annotations = pd.read_csv('../../data/annotations/clean/annotations.tsv', sep='\\t')\n",
    "all_annotations.index = all_annotations.rev_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "model_name = 'linear_char_ed_train'\n",
    "model_type = 'linear_char_ed'\n",
    "\n",
    "tasks = ['aggression', 'attack', 'recipient']\n",
    "\n",
    "model_dict = {}\n",
    "calibrator_dict = {}\n",
    "\n",
    "for task in tasks:\n",
    "    path = '../../models/%s/%s' % (task, model_type)\n",
    "    model_dict[task] = load_pipeline(path, model_name)\n",
    "    calibrator_dict[task] = joblib.load(os.path.join(path, 'calibrator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.29 ms\n"
     ]
    }
   ],
   "source": [
    "def apply_models(df):\n",
    "    diffs = df['clean_diff']\n",
    "    for task, model in model_dict.items():\n",
    "        scores = model.predict_proba(diffs)[:,1]\n",
    "        df['pred_%s_score_uncalibrated' % task] = scores\n",
    "        df['pred_%s_score_calibrated' % task] = calibrator_dict[task].transform(scores)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load annotationed diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "cols = ['rev_id', 'ns', 'sample', 'src', 'clean_diff', 'diff', 'page_id', 'page_title', 'rev_comment', 'rev_timestamp', 'user_id', 'user_text']\n",
    "for ns in ['user', 'article']:\n",
    "\n",
    "    d_annotations = all_annotations.query(\"sample=='random' and ns=='%s'\" % ns)\n",
    "    d_annotated = d_annotations\\\n",
    "                .drop_duplicates(subset=['rev_id'])[cols]\\\n",
    "                .assign(\n",
    "                    recipient = plurality(d_annotations['recipient'].dropna()),\n",
    "                    recipient_score = average(d_annotations['recipient'].dropna()),\n",
    "                    aggression = plurality(d_annotations['aggression'].dropna()),\n",
    "                    aggression_score = average(d_annotations['aggression'].dropna()),\n",
    "                    attack = plurality(d_annotations['attack'].dropna()),\n",
    "                    attack_score = average(d_annotations['attack'].dropna())\n",
    "            )\n",
    "\n",
    "    d_annotated.to_csv('../../data/samples/%s/clean/d_annotated.tsv' % ns, sep = '\\t', index = False)\n",
    "del all_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load samples and apply models\n",
    "\n",
    "We take various diffs datasets from hive, apply the clean and filter function and the score the clean diffs using the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.8 ms\n"
     ]
    }
   ],
   "source": [
    "def pred_helper(df):\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    return df.assign(rev_timestamp = lambda x: pd.to_datetime(x.rev_timestamp),\n",
    "                     clean_diff = lambda x: x['clean_diff'].astype(str))\\\n",
    "             .pipe(apply_models)\n",
    "\n",
    "    \n",
    "def prep_in_parallel(path, k = 8):\n",
    "    df = pd.read_csv(path, sep = '\\t', encoding = 'utf-8')\n",
    "    m = df.shape[0] \n",
    "    n_groups = int(m / 10000.0)\n",
    "    df['key'] = np.random.randint(0, high=n_groups, size=m)\n",
    "    dfs = [e[1] for e in df.groupby('key')]\n",
    "    dfs = [pred_helper(d) for d in dfs]\n",
    "    #p = mp.Pool(k)\n",
    "    #dfs = p.map(pred_helper, dfs)\n",
    "    #p.close()\n",
    "    #p.join()\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/samples/user/clean/d_annotated.tsv\n",
      "../../data/samples/user/clean/talk_diff_no_admin_sample.tsv\n",
      "../../data/samples/user/clean/talk_diff_no_admin_2015.tsv\n",
      "../../data/samples/user/clean/all_blocked_user.tsv\n",
      "../../data/samples/article/clean/d_annotated.tsv\n",
      "../../data/samples/article/clean/talk_diff_no_admin_sample.tsv\n",
      "../../data/samples/article/clean/talk_diff_no_admin_2015.tsv\n",
      "../../data/samples/article/clean/all_blocked_user.tsv\n",
      "time: 8h 32min 10s\n"
     ]
    }
   ],
   "source": [
    "base = '../../data/samples/'\n",
    "nss = ['user', 'article']\n",
    "samples = ['d_annotated.tsv', 'talk_diff_no_admin_sample.tsv', 'talk_diff_no_admin_2015.tsv', 'all_blocked_user.tsv']\n",
    "\n",
    "base_cols = ['rev_id',\n",
    "             'clean_diff',\n",
    "             'rev_timestamp',\n",
    "             'pred_aggression_score_uncalibrated',\n",
    "             'pred_recipient_score_uncalibrated',\n",
    "             'pred_attack_score_uncalibrated',\n",
    "             'pred_aggression_score_calibrated',\n",
    "             'pred_recipient_score_calibrated',\n",
    "             'pred_attack_score_calibrated',\n",
    "             'page_title',\n",
    "             'user_text',\n",
    "             'user_id'\n",
    "            ]\n",
    "\n",
    "extra_cols = ['recipient', 'recipient_score', 'aggression', 'aggression_score', 'attack', 'attack_score']\n",
    "\n",
    "for ns in nss:\n",
    "    for s in samples:\n",
    "        inf = os.path.join(base, ns, 'clean', s)\n",
    "        print(inf)\n",
    "        outf = os.path.join(base, ns, 'scored', s)\n",
    "        if s == 'd_annotated.tsv':\n",
    "            cols = base_cols + extra_cols\n",
    "        else:\n",
    "            cols = base_cols\n",
    "        prep_in_parallel(inf, k = 4)[cols].to_csv(outf, sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
